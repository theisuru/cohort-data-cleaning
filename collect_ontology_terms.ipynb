{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "download MONDO ontology and save its terms to a file.\n",
    "\n",
    "`curl -L 'http://www.ebi.ac.uk/ols/api/ontologies/mondo/terms' -i -H 'Accept: application/json'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "['extracellular ligand-gated ion channel activity',\n 'ligand-gated ion channel activity',\n 'excitatory extracellular ligand-gated ion channel activity',\n 'L1CAM',\n 'gene']"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "terms = []\n",
    "paging = {'page': 0, 'size': 5}\n",
    "headers = {'Accept': 'application/json'}\n",
    "r = requests.get('http://www.ebi.ac.uk/ols/api/ontologies/mondo/terms', params=paging, headers=headers)\n",
    "if r.raise_for_status():\n",
    "    print(\"Error on page: \" + str(paging))\n",
    "response = r.json()\n",
    "terms.extend(x['label'] for x in response['_embedded']['terms'])\n",
    "terms"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pages: 222\n",
      "Page retrieved: 0\n",
      "Page retrieved: 1\n",
      "Page retrieved: 2\n",
      "Page retrieved: 3\n",
      "Page retrieved: 4\n",
      "Page retrieved: 5\n",
      "Page retrieved: 6\n",
      "Page retrieved: 7\n",
      "Page retrieved: 8\n",
      "Page retrieved: 9\n",
      "Page retrieved: 10\n",
      "Page retrieved: 11\n",
      "Page retrieved: 12\n",
      "Page retrieved: 13\n",
      "Page retrieved: 14\n",
      "Page retrieved: 15\n",
      "Page retrieved: 16\n",
      "Page retrieved: 17\n",
      "Page retrieved: 18\n",
      "Page retrieved: 19\n",
      "Page retrieved: 20\n",
      "Page retrieved: 21\n",
      "Page retrieved: 22\n",
      "Page retrieved: 23\n",
      "Page retrieved: 24\n",
      "Page retrieved: 25\n",
      "Page retrieved: 26\n",
      "Page retrieved: 27\n",
      "Page retrieved: 28\n",
      "Page retrieved: 29\n",
      "Page retrieved: 30\n",
      "Page retrieved: 31\n",
      "Page retrieved: 32\n",
      "Page retrieved: 33\n",
      "Page retrieved: 34\n",
      "Page retrieved: 35\n",
      "Page retrieved: 36\n",
      "Page retrieved: 37\n",
      "Page retrieved: 38\n",
      "Page retrieved: 39\n",
      "Page retrieved: 40\n",
      "Page retrieved: 41\n",
      "Page retrieved: 42\n",
      "Page retrieved: 43\n",
      "Page retrieved: 44\n",
      "Page retrieved: 45\n",
      "Page retrieved: 46\n",
      "Page retrieved: 47\n",
      "Page retrieved: 48\n",
      "Page retrieved: 49\n",
      "Page retrieved: 50\n",
      "Page retrieved: 51\n",
      "Page retrieved: 52\n",
      "Page retrieved: 53\n",
      "Page retrieved: 54\n",
      "Page retrieved: 55\n",
      "Page retrieved: 56\n",
      "Page retrieved: 57\n",
      "Page retrieved: 58\n",
      "Page retrieved: 59\n",
      "Page retrieved: 60\n",
      "Page retrieved: 61\n",
      "Page retrieved: 62\n",
      "Page retrieved: 63\n",
      "Page retrieved: 64\n",
      "Page retrieved: 65\n",
      "Page retrieved: 66\n",
      "Page retrieved: 67\n",
      "Page retrieved: 68\n",
      "Page retrieved: 69\n",
      "Page retrieved: 70\n",
      "Page retrieved: 71\n",
      "Page retrieved: 72\n",
      "Page retrieved: 73\n",
      "Page retrieved: 74\n",
      "Page retrieved: 75\n",
      "Page retrieved: 76\n",
      "Page retrieved: 77\n",
      "Page retrieved: 78\n",
      "Page retrieved: 79\n",
      "Page retrieved: 80\n",
      "Page retrieved: 81\n",
      "Page retrieved: 82\n",
      "Page retrieved: 83\n",
      "Page retrieved: 84\n",
      "Page retrieved: 85\n",
      "Page retrieved: 86\n",
      "Page retrieved: 87\n",
      "Page retrieved: 88\n",
      "Page retrieved: 89\n",
      "Page retrieved: 90\n",
      "Page retrieved: 91\n",
      "Page retrieved: 92\n",
      "Page retrieved: 93\n",
      "Page retrieved: 94\n",
      "Page retrieved: 95\n",
      "Page retrieved: 96\n",
      "Page retrieved: 97\n",
      "Page retrieved: 98\n",
      "Page retrieved: 99\n",
      "Page retrieved: 100\n",
      "Page retrieved: 101\n",
      "Page retrieved: 102\n",
      "Page retrieved: 103\n",
      "Page retrieved: 104\n",
      "Page retrieved: 105\n",
      "Page retrieved: 106\n",
      "Page retrieved: 107\n",
      "Page retrieved: 108\n",
      "Page retrieved: 109\n",
      "Page retrieved: 110\n",
      "Page retrieved: 111\n",
      "Page retrieved: 112\n",
      "Page retrieved: 113\n",
      "Page retrieved: 114\n",
      "Page retrieved: 115\n",
      "Page retrieved: 116\n",
      "Page retrieved: 117\n",
      "Page retrieved: 118\n",
      "Page retrieved: 119\n",
      "Page retrieved: 120\n",
      "Page retrieved: 121\n",
      "Page retrieved: 122\n",
      "Page retrieved: 123\n",
      "Page retrieved: 124\n",
      "Page retrieved: 125\n",
      "Page retrieved: 126\n",
      "Page retrieved: 127\n",
      "Page retrieved: 128\n",
      "Page retrieved: 129\n",
      "Page retrieved: 130\n",
      "Page retrieved: 131\n",
      "Page retrieved: 132\n",
      "Page retrieved: 133\n",
      "Page retrieved: 134\n",
      "Page retrieved: 135\n",
      "Page retrieved: 136\n",
      "Page retrieved: 137\n",
      "Page retrieved: 138\n",
      "Page retrieved: 139\n",
      "Page retrieved: 140\n",
      "Page retrieved: 141\n",
      "Page retrieved: 142\n",
      "Page retrieved: 143\n",
      "Page retrieved: 144\n",
      "Page retrieved: 145\n",
      "Page retrieved: 146\n",
      "Page retrieved: 147\n",
      "Page retrieved: 148\n",
      "Page retrieved: 149\n",
      "Page retrieved: 150\n",
      "Page retrieved: 151\n",
      "Page retrieved: 152\n",
      "Page retrieved: 153\n",
      "Page retrieved: 154\n",
      "Page retrieved: 155\n",
      "Page retrieved: 156\n",
      "Page retrieved: 157\n",
      "Page retrieved: 158\n",
      "Page retrieved: 159\n",
      "Page retrieved: 160\n",
      "Page retrieved: 161\n",
      "Page retrieved: 162\n",
      "Page retrieved: 163\n",
      "Page retrieved: 164\n",
      "Page retrieved: 165\n",
      "Page retrieved: 166\n",
      "Page retrieved: 167\n",
      "Page retrieved: 168\n",
      "Page retrieved: 169\n",
      "Page retrieved: 170\n",
      "Page retrieved: 171\n",
      "Page retrieved: 172\n",
      "Page retrieved: 173\n",
      "Page retrieved: 174\n",
      "Page retrieved: 175\n",
      "Page retrieved: 176\n",
      "Page retrieved: 177\n",
      "Page retrieved: 178\n",
      "Page retrieved: 179\n",
      "Page retrieved: 180\n",
      "Page retrieved: 181\n",
      "Page retrieved: 182\n",
      "Page retrieved: 183\n",
      "Page retrieved: 184\n",
      "Page retrieved: 185\n",
      "Page retrieved: 186\n",
      "Page retrieved: 187\n",
      "Page retrieved: 188\n",
      "Page retrieved: 189\n",
      "Page retrieved: 190\n",
      "Page retrieved: 191\n",
      "Page retrieved: 192\n",
      "Page retrieved: 193\n",
      "Page retrieved: 194\n",
      "Page retrieved: 195\n",
      "Page retrieved: 196\n",
      "Page retrieved: 197\n",
      "Page retrieved: 198\n",
      "Page retrieved: 199\n",
      "Page retrieved: 200\n",
      "Page retrieved: 201\n",
      "Page retrieved: 202\n",
      "Page retrieved: 203\n",
      "Page retrieved: 204\n",
      "Page retrieved: 205\n",
      "Page retrieved: 206\n",
      "Page retrieved: 207\n",
      "Page retrieved: 208\n",
      "Page retrieved: 209\n",
      "Page retrieved: 210\n",
      "Page retrieved: 211\n",
      "Page retrieved: 212\n",
      "Page retrieved: 213\n",
      "Page retrieved: 214\n",
      "Page retrieved: 215\n",
      "Page retrieved: 216\n",
      "Page retrieved: 217\n",
      "Page retrieved: 218\n",
      "Page retrieved: 219\n",
      "Page retrieved: 220\n",
      "Page retrieved: 221\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "page = 1\n",
    "size = 200\n",
    "paging = {'page': page, 'size': size}\n",
    "headers = {'Accept': 'application/json'}\n",
    "r = requests.get('http://www.ebi.ac.uk/ols/api/ontologies/mondo/terms', params=paging, headers=headers)\n",
    "response = r.json()\n",
    "total_pages = response['page']['totalPages']\n",
    "print(\"Total number of pages: \" + str(total_pages))\n",
    "\n",
    "terms = []\n",
    "for x in range(total_pages):\n",
    "    paging = {'page': x, 'size': size}\n",
    "    r = requests.get('http://www.ebi.ac.uk/ols/api/ontologies/mondo/terms', params=paging)\n",
    "    response = r.json()\n",
    "    print(\"Page retrieved: \" + str(x))\n",
    "    terms.extend(x['label'] for x in response['_embedded']['terms'])\n",
    "\n",
    "mondo_terms = pd.DataFrame(terms)\n",
    "mondo_terms.to_csv('data/mondo_vocabulary.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "African, African, 1.0\n",
      "British, British, 1.0\n",
      "Asian or Asian British, Asian or Asian British, 1.0\n",
      "White and Asian, White and Asian, 1.0\n",
      "White and Black African, White and Black African, 1.0\n",
      "Prefer not to answer, Prefer not to answer, 1.0\n",
      "Any other Asian background, Any other Asian background, 1.0\n",
      "White and Black Caribbean, White and Black Caribbean, 1.0\n",
      "Black or Black British, Black or Black British, 1.0\n",
      "Any other mixed background, Any other mixed background, 1.0\n",
      "Bangladeshi, Bangladeshi, 1.0\n",
      "Do not know, Do not know, 1.0\n",
      "Asian kr Asiao British, Asian or Asian British, 0.9090909090909091\n",
      "Any other Black background, Any other Black background, 1.0\n",
      "Irish, Irish, 1.0\n",
      "Mixed, Mixed, 1.0\n",
      "White, White, 1.0\n",
      "Pakistani, Pakistani, 1.0\n",
      "Caribjcan, Caribbean, 0.7777777777777778\n",
      "White and Black Caribbeau, White and Black Caribbean, 0.96\n",
      "Other ethnic group, Other ethnic group, 1.0\n",
      "Indian, Indian, 1.0\n",
      "Brktisr, British, 0.7142857142857143\n",
      "Any other white background, Any other white background, 1.0\n",
      "Chvnese, Chinese, 0.8571428571428571\n",
      "Caribbean, Caribbean, 1.0\n",
      "Chinese, Chinese, 1.0\n",
      "White anh Askan, White and Asian, 0.8666666666666667\n",
      "Pakisttnx, Pakistani, 0.7777777777777778\n",
      "Any other mixod bacoground, Any other mixed background, 0.9230769230769231\n",
      "zritush, British, 0.7142857142857143\n",
      "Biack or tlack British, Black or Black British, 0.9090909090909091\n",
      "Bvitisn, British, 0.7142857142857143\n",
      "Banoradeshi, Bangladeshi, 0.8181818181818182\n",
      "Bnitisl, British, 0.7142857142857143\n",
      "wangladsshi, Bangladeshi, 0.8181818181818182\n",
      "Black orbBlackmBritish, Black or Black British, 0.9090909090909091\n",
      "White and Black Caribbeie, White and Black Caribbean, 0.92\n",
      "Injwan, Indian, 0.6666666666666666\n",
      "Panistuni, Pakistani, 0.4444444444444444\n",
      "Bleck or Black Bratish, Black or Black British, 0.9090909090909091\n",
      "Mhxej, Mixed, 0.6\n",
      "Bsndladeshi, Bangladeshi, 0.8181818181818182\n",
      "Brirksh, British, 0.7142857142857143\n",
      "Auy other Asian baukground, Any other Asian background, 0.9230769230769231\n",
      "takmstani, Pakistani, 0.7777777777777778\n",
      "Africjn, African, 0.8571428571428571\n",
      "Ayy otherfBlack background, Any other Black background, 0.9230769230769231\n",
      "Whitz and Black Cwribbean, White and Black Caribbean, 0.92\n",
      "Brrtash, British, 0.7142857142857143\n",
      "Anycotheq white background, Any other white background, 0.9230769230769231\n",
      "Bpnghadeshi, Bangladeshi, 0.8181818181818182\n",
      "Mipyd, Mixed, 0.6\n",
      "gixdd, Mixed, 0.6\n",
      "Indfan, Indian, 0.8333333333333334\n",
      "qaribbean, Caribbean, 0.8888888888888888\n",
      "Pakistsei, Pakistani, 0.7777777777777778\n",
      "Black oreBlack Bribish, Black or Black British, 0.9090909090909091\n",
      "Micem, Mixed, 0.6\n",
      "Ijilh, Irish, 0.6\n",
      "White and Blahk Africat, White and Black African, 0.9130434782608695\n",
      "Whitpzand Asian, White and Asian, 0.8666666666666667\n",
      "Asian or Asian kritisi, Asian or Asian British, 0.9090909090909091\n",
      "Pamisuani, Pakistani, 0.7777777777777778\n",
      "Iwisb, Irish, 0.6\n",
      "Wyitj and Black Caribbean, White and Black Caribbean, 0.92\n",
      "gakiltani, Pakistani, 0.7777777777777778\n",
      "crish, Irish, 0.8\n",
      "lhite, White, 0.8\n",
      "qther ethnic groua, Other ethnic group, 0.8888888888888888\n",
      "Wnitn, White, 0.6\n",
      "Blajk op Black British, Black or Black British, 0.9090909090909091\n",
      "Iridq, Irish, 0.6\n",
      "Bgitisq, British, 0.7142857142857143\n",
      "Caribbwap, Caribbean, 0.7777777777777778\n",
      "Carwbbman, Caribbean, 0.7777777777777778\n",
      "Whith, White, 0.8\n",
      "Anw otheo mixed background, Any other mixed background, 0.9230769230769231\n",
      "Donnot knoo, Do not know, 0.8181818181818182\n",
      "Bangvxdeshi, Bangladeshi, 0.8181818181818182\n",
      "White pnd xlack Caribbean, White and Black Caribbean, 0.92\n",
      "Other ethnic vvoup, Other ethnic group, 0.8888888888888888\n",
      "fndias, Indian, 0.6666666666666666\n",
      "White and xsian, White and Asian, 0.9333333333333333\n",
      "Black or Blabk British, Black or Black British, 0.9545454545454546\n",
      "Whhxe and Black African, White and Black African, 0.9130434782608695\n",
      "Any otheraAsian backhround, Any other Asian background, 0.9230769230769231\n",
      "Any otaer Block background, Any other Black background, 0.9230769230769231\n",
      "Wuide, White, 0.6\n",
      "Mixeg, Mixed, 0.8\n",
      "Afrnqan, African, 0.7142857142857143\n",
      "Othmr etinic group, Other ethnic group, 0.8888888888888888\n",
      "ehite, White, 0.8\n",
      "Imysh, Irish, 0.6\n",
      "Prefzr notkto answer, Prefer not to answer, 0.9\n",
      "Waite, White, 0.8\n",
      "Anj opher mixed background, Any other mixed background, 0.9230769230769231\n",
      "Ady other Black backgroung, Any other Black background, 0.9230769230769231\n",
      "Aiy other Black uackground, Any other Black background, 0.9230769230769231\n",
      "tixod, Mixed, 0.6\n",
      "Asiansor AsiannBritish, Asian or Asian British, 0.9090909090909091\n",
      "khipe, White, 0.6\n",
      "Any other Black backgrohnd, Any other Black background, 0.9615384615384616\n",
      "Any other whitelbacmground, Any other white background, 0.9230769230769231\n",
      "White ant Black Cahibbean, White and Black Caribbean, 0.92\n",
      "Whete and Black Afrtcan, White and Black African, 0.9130434782608695\n",
      "Blackbor Black Britijh, Black or Black British, 0.9090909090909091\n",
      "jhqnese, Chinese, 0.7142857142857143\n",
      "White and Black Carirbxan, White and Black Caribbean, 0.92\n",
      "Carqbbean, Caribbean, 0.8888888888888888\n",
      "Banglndesoi, Bangladeshi, 0.8181818181818182\n",
      "Do notmfnow, Do not know, 0.8181818181818182\n",
      "Whmtb, White, 0.6\n",
      "Asran or Asian Britirh, Asian or Asian British, 0.9090909090909091\n",
      "Any other Blacj bkckground, Any other Black background, 0.9230769230769231\n",
      "thite cnd Black African, White and Black African, 0.9130434782608695\n",
      "Any other Asian baclzround, Any other Asian background, 0.9230769230769231\n",
      "Asian op Asian Briiish, Asian or Asian British, 0.9090909090909091\n",
      "Othee ethnic group, Other ethnic group, 0.9444444444444444\n",
      "srivh, Irish, 0.6\n",
      "Inskan, Indian, 0.6666666666666666\n",
      "Asian od Asian Briyish, Asian or Asian British, 0.9090909090909091\n",
      "Any bther Asianfbackground, Any other Asian background, 0.9230769230769231\n",
      "iixzd, Mixed, 0.6\n",
      "Mixud, Mixed, 0.8\n",
      "Aey other white backgrovnd, Any other white background, 0.9230769230769231\n",
      "Prefer not tooanswey, Prefer not to answer, 0.9\n",
      "Any other bixed bankground, Any other mixed background, 0.9230769230769231\n",
      "Black or Blacs Britirh, Black or Black British, 0.9090909090909091\n",
      "Ant oiher Asian background, Any other Asian background, 0.9230769230769231\n",
      "Cariboean, Caribbean, 0.8888888888888888\n",
      "Dvnnot know, Do not know, 0.8181818181818182\n",
      "Obher ethnicagroup, Other ethnic group, 0.8888888888888888\n",
      "Chgndse, Chinese, 0.7142857142857143\n",
      "lbish, Irish, 0.6\n",
      "Any other Aqpan background, Any other Asian background, 0.9230769230769231\n",
      "Africad, African, 0.8571428571428571\n",
      "Itksh, Irish, 0.6\n",
      "White and Blvck Afrikan, White and Black African, 0.9130434782608695\n",
      "White and BlyckkAfrican, White and Black African, 0.9130434782608695\n",
      "Cariebean, Caribbean, 0.8888888888888888\n",
      "Any otheriAsian qackground, Any other Asian background, 0.9230769230769231\n",
      "White and Blach Caribbean, White and Black Caribbean, 0.96\n",
      "Mqked, Mixed, 0.6\n",
      "Asian or Asiin British, Asian or Asian British, 0.9545454545454546\n",
      "otiak or Asiqn British, Asian or Asian British, 0.8181818181818182\n",
      "White and Blacp ofrican, White and Black African, 0.9130434782608695\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "ukb_data = pd.read_csv('data/ukb_synthetic_data_tofu.csv')\n",
    "# values = ukb_data['Ethnic background-0.0'].unique()\n",
    "\n",
    "ukb_data_noise = pd.read_csv('data/ukb_synthetic_data_with_noise.csv')\n",
    "values_noise = ukb_data_noise['Ethnic background-0.0'].unique()\n",
    "\n",
    "values = ['African', 'British', 'Asian or Asian British', 'White and Asian',\n",
    "       'White and Black African', 'Prefer not to answer',\n",
    "       'Any other Asian background', 'White and Black Caribbean',\n",
    "       'Black or Black British', 'Any other mixed background',\n",
    "       'Bangladeshi', 'Do not know', 'Any other Black background',\n",
    "       'Irish', 'Mixed', 'White', 'Pakistani', 'Caribbean',\n",
    "       'Other ethnic group', 'Indian', 'Any other white background',\n",
    "       'Chinese']\n",
    "\n",
    "for val in values_noise:\n",
    "    # print(max([SequenceMatcher(None, val, x).ratio() for x in values]))\n",
    "    max_match = [SequenceMatcher(None, val, x).ratio() for x in values]\n",
    "    best_match = values[np.argmax(max_match)]\n",
    "    print(val + \", \" + best_match + \", \" + str(max(max_match)))\n",
    "    # print(SequenceMatcher(None, val, values[0]).ratio())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "{'a': 1, 'b': 2, 'c': 3}"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "ethnic_b_values_noise = ukb_data['ethnic background'].unique()\n",
    "\n",
    "ethnic_b_values = ['African', 'British', 'Asian or Asian British', 'White and Asian',\n",
    "       'White and Black African', 'Prefer not to answer',\n",
    "       'Any other Asian background', 'White and Black Caribbean',\n",
    "       'Black or Black British', 'Any other mixed background',\n",
    "       'Bangladeshi', 'Do not know', 'Any other Black background',\n",
    "       'Irish', 'Mixed', 'White', 'Pakistani', 'Caribbean',\n",
    "       'Other ethnic group', 'Indian', 'Any other white background',\n",
    "       'Chinese']\n",
    "\n",
    "similarity_list = []\n",
    "for val in ethnic_b_values_noise:\n",
    "    similarity = [SequenceMatcher(None, val, x).ratio() for x in ethnic_b_values]\n",
    "    best_match = ethnic_b_values[np.argmax(similarity)]\n",
    "    ratio = max(similarity)\n",
    "    if ratio < 1:\n",
    "        similarity_list.append({'value': val, 'best_match': best_match, 'ratio': ratio})\n",
    "\n",
    "pd.DataFrame(similarity_list).head(20)\n",
    "\n",
    "ethnic_b_mappings = {match['value']: match['best_match'] for match in similarity_list}\n",
    "ethnic_b_mappings = {**ethnic_b_mappings, **{i:i for i in ethnic_b_values}}\n",
    "\n",
    "ukb_data['ethnic background'] = ukb_data['ethnic background'].map(ethnic_b_mappings)\n",
    "pd.DataFrame(ukb_data['ethnic background'].unique(), columns=['values'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cant', 'railway'), ('railway', 'station')]\n",
      "[('citadel', 'hotel')]\n",
      "[('police', ','), (',', 's'), ('s', 'stn')]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "# nltk.download('punkt')\n",
    "\n",
    "text = ['cant railway station', 'citadel hotel', 'police stn']\n",
    "for line in text:\n",
    "    token = word_tokenize(line)\n",
    "    bigram = list(ngrams(token, 2))\n",
    "    print(bigram)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "                    0             1        2\n0       extracellular  ligand-gated  1000000\n1        ligand-gated           ion  1000000\n2                 ion       channel  1000000\n3             channel      activity  1000000\n4        ligand-gated           ion  1000000\n...               ...           ...      ...\n113016        sensory           and  1000000\n113017            and     autonomic  1000000\n113018      autonomic    neuropathy  1000000\n113019     neuropathy          type  1000000\n113020           type             6  1000000\n\n[113021 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>extracellular</td>\n      <td>ligand-gated</td>\n      <td>1000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ligand-gated</td>\n      <td>ion</td>\n      <td>1000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ion</td>\n      <td>channel</td>\n      <td>1000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>channel</td>\n      <td>activity</td>\n      <td>1000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ligand-gated</td>\n      <td>ion</td>\n      <td>1000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>113016</th>\n      <td>sensory</td>\n      <td>and</td>\n      <td>1000000</td>\n    </tr>\n    <tr>\n      <th>113017</th>\n      <td>and</td>\n      <td>autonomic</td>\n      <td>1000000</td>\n    </tr>\n    <tr>\n      <th>113018</th>\n      <td>autonomic</td>\n      <td>neuropathy</td>\n      <td>1000000</td>\n    </tr>\n    <tr>\n      <th>113019</th>\n      <td>neuropathy</td>\n      <td>type</td>\n      <td>1000000</td>\n    </tr>\n    <tr>\n      <th>113020</th>\n      <td>type</td>\n      <td>6</td>\n      <td>1000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>113021 rows Ã— 3 columns</p>\n</div>"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "mondo_terms = pd.read_csv('data/mondo_vocabulary.csv')\n",
    "mondo_bigrams = []\n",
    "for index, row in mondo_terms.iterrows():\n",
    "    bigrams = list(ngrams(word_tokenize(str(row['TERM'])), 2))\n",
    "    mondo_bigrams.extend(bigrams)\n",
    "\n",
    "bigram_df = pd.DataFrame(mondo_bigrams)\n",
    "bigram_df[2] = 1000000\n",
    "bigram_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(242342, 3)\n"
     ]
    }
   ],
   "source": [
    "bigrams = pd.read_csv('data/frequency_bigramdictionary_en_243_342.txt', sep=\" \", header=None)\n",
    "print(bigrams.shape)\n",
    "all_bigrams = pd.concat([bigrams, bigram_df])\n",
    "\n",
    "all_bigrams.to_csv('data/frequency_bigramdictionary_with_mondo.txt', sep=\" \", header=None, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "mondo_terms = pd.read_csv('data/mondo_vocabulary.csv')\n",
    "\n",
    "tokenized_terms = []\n",
    "for index, row in mondo_terms.iterrows():\n",
    "    tokenized_terms.extend(list(ngrams(word_tokenize(str(row['TERM'])), 1)))\n",
    "\n",
    "mondo_terms = pd.DataFrame(tokenized_terms)\n",
    "dic_terms = pd.read_csv('data/frequency_dictionary_en_82_765.txt', sep=\" \", header=None)\n",
    "mondo_terms[1] = 100000000\n",
    "all_terms = pd.concat([dic_terms, mondo_terms])\n",
    "all_terms.to_csv('data/frequency_dictionary_with_mondo.txt', sep=\" \", header=None, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}