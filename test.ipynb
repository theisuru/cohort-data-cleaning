{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Are we doing any of\n",
    "1. lower casing\n",
    "2. removing stop words\n",
    "3. removing punctuations\n",
    "4. removing extra spaces\n",
    "5. stem and lemmatization\n",
    "\n",
    "Take the syn data excerpt from EGA and do NER? paragraph cleaning\n",
    "\n",
    "map each column to Gecko\n",
    "then select one column and get a mapping field in another cohort\n",
    "\n",
    "https://www.ebi.ac.uk/ols/api/ontologies/efo/terms?page=0&size=10\n",
    "NCIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ukb_data = pd.read_csv(\"data/ukb_synthetic_data_tofu_noise.csv\")\n",
    "ukb_data['Sex'] = ukb_data['Sex'].map({'female': 0, 'male': 1})\n",
    "\n",
    "ukb_data.to_csv(\"data/ukb_synthetic_data_tofu_noise_and_encoded.csv\", index=False, encoding='utf-8')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.pie(ukb_syn_pd['Body mass index (BMI)-0.0'])\n",
    "plt.show()\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "words = [\"jump\", \"jumped\", \"jumps\", \"jumping\"]\n",
    "stemmer = PorterStemmer()\n",
    "for word in words:\n",
    "    print(word + \" = \" + stemmer.stem(word))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import nltk.corpus\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "text = \"my package from amazon never arrived fix this asap\"\n",
    "text = \" \".join([word for word in text.split() if word not in stop])\n",
    "\n",
    "print(text)\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "text = \"hey amazon - my package never arrived https://www.amazon.com/gp/css/order-history?ref_=nav_orders_first please fix asap! @amazonhelp\"\n",
    "text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", text)\n",
    "print(text)\n",
    "\n",
    "\n",
    "import requests\n",
    "r = requests.get('https://www.ebi.ac.uk/ols/api/ontologies/efo/terms?page=0&size=10')\n",
    "r.json()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}